# Getting Started with CrowdTruth

## Installation

To install the stable version from PyPI, install *pip* for your OS, then install package using:
```
pip install crowdtruth
```

To install the latest version from source, download the library and install it using:
```
python setup.py install
```


## How to run

After installing the CrowdTruth package, you can run the metrics on your own crowdsourced data. We currently support automated processing of files generated by [Amazon Mechanical Turk](https://www.mturk.com/) and [Figure Eight](https://www.figure-eight.com/). It is also possible to define your own [custom file format](TODO:add_link).

### 1. Define the configuration

The pre-processing configuration defines how to interpret the raw crowdsourcing input. To do this, we need to define a configuration class.

```
import crowdtruth
from crowdtruth.configuration import DefaultConfig

class TestConfig(DefaultConfig):
  ...
```

Our test class inherits the default configuration `DefaultConfig`. The following attributes can be used to customize the configuration to the task:

* **`inputColumns`:** list of input columns from the .csv file with the input data
* **`outputColumns`:** list of output columns from the .csv file with the answers from the workers
* **`csv_file_separator`:** string that separates between the columns in the file, default value is `,`
* **`annotation_separator`:** string that separates between the crowd annotations (the columns defined in `outputColumns`), default value is `,`
* **`open_ended_task`:** boolean variable defining whether the task is open-ended (i.e. the possible crowd annotations are not known beforehand, like in the case of free text input) or not (i.e. the crowd picks from a pre-selected list of annotations)
* **`annotation_vector`:** list of possible crowd answers, obligatory when `open_ended_task` is `False`
* **`processJudgments`:** method that defines additional processing of the raw crowd data


### 2. Pre-process the data

After declaring the configuration of our input file, we are ready to pre-process the crowd data:

```
data, config = crowdtruth.load(
    file = ...,
    config = TestConfig()
)
```

To process all of the files in one folder with the same pre-defined configuration, replace the `file` attribute of `crowdtruth.load` with `directory`.


### 3. Calculate the metrics

The pre-processed data can then be used to calculate the CrowdTruth metrics:

```
results = crowdtruth.run(data, config)
```

The `crowdtruth.run` method returns a dictionary object with the following keys:

* `units`: quality metrics for the input units
* `workers`: quality metrics for the workers
* `annotations`: quality metrics for the crowd annotations

## Example tasks

Below you can find a collection of Jupyter Notebooks that show how to use the CrowdTruth package on different types of crowdsourcing tasks:

**Binary choice tasks:** the crowd picks 1 annotation out of 2 choices (e.g. `True` and `False`)

* [Person identification in videos](https://github.com/CrowdTruth/CrowdTruth-core/blob/master/tutorial/Binary%20Choice%20Task%20-%20Person%20Identification%20in%20Video.ipynb)
* [Relation extraction from sentences](https://github.com/CrowdTruth/CrowdTruth-core/blob/master/tutorial/Binary%20Choice%20Task%20-%20Relation%20Extraction.ipynb)

**Ternary choice tasks:** the crowd picks 1 annotation out of 3 choices, (e.g. `True`, `False` and `None/Other`)

* [Person identification in videos](https://github.com/CrowdTruth/CrowdTruth-core/blob/master/tutorial/Ternary%20Choice%20Task%20-%20Person%20Annotation%20in%20Video.ipynb)

**Multiple choice tasks:** the crowd picks multiple annotation out of a set list of choices that are *the same* for every input unit

* [Person identification in videos](https://github.com/CrowdTruth/CrowdTruth-core/blob/master/tutorial/Multiple%20Choice%20Task%20-%20Person%20Type%20(Role)%20Annotation%20in%20Video.ipynb)
* [Relation extraction from sentences](https://github.com/CrowdTruth/CrowdTruth-core/blob/master/tutorial/Multiple%20Choice%20Task%20-%20Relation%20Extraction.ipynb)

**Sparse multiple choice tasks:** the crowd picks multiple annotation out of a set list of choices that are *different* across input units

* [Person identification in videos](https://github.com/CrowdTruth/CrowdTruth-core/blob/master/tutorial/Sparse%20Choice%20Task%20-%20Person%20Annotation%20in%20Video.ipynb)
* [Relation extraction from sentences](https://github.com/CrowdTruth/CrowdTruth-core/blob/master/tutorial/Sparse%20Multiple%20Choice%20Task%20-%20Relation%20Extraction.ipynb)

**Open-ended extraction tasks:** the crowd creates different combinations of annotations based on the input unit (e.g. extracting named entities from a paragraph, identifying objects in an image by drawing bounding boxes)

**Free input:** the crowd inputs all possible annotations for an input unit
